{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 921
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "tFHKCKuGJrcl",
    "outputId": "a35f47cb-4b8b-460b-f66d-31171f8ce9d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras as ks\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1037
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "yyQYaMKTJ33G",
    "outputId": "65aa8153-e3c0-4a66-d37c-254a34a36323"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練画像\n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 003 018 018 018 126 136 175 026 166 255 247 127 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 030 036 094 154 170 253 253 253 253 253 225 172 253 242 195 064 000 000 000 000 \n",
      "000 000 000 000 000 000 000 049 238 253 253 253 253 253 253 253 253 251 093 082 082 056 039 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 018 219 253 253 253 253 253 198 182 247 241 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 080 156 107 253 253 205 011 000 043 154 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 014 001 154 253 090 000 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 139 253 190 002 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 011 190 253 070 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 035 241 225 160 108 001 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 081 240 253 253 119 025 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 045 186 253 253 150 027 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 016 093 252 253 187 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 249 253 249 064 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 046 130 183 253 253 207 002 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 039 148 229 253 253 253 250 182 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 024 114 221 253 253 253 253 201 078 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 023 066 213 253 253 253 253 198 081 002 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 018 171 219 253 253 253 253 195 080 009 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 055 172 226 253 253 253 253 244 133 011 000 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 136 253 253 253 212 135 132 016 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "訓練ラベル (y_train) = 5\n",
      "テスト画像\n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 084 185 159 151 060 036 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 222 254 254 254 254 241 198 198 198 198 198 198 198 198 170 052 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 067 114 072 114 163 227 254 225 254 254 254 250 229 254 254 140 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 017 066 014 067 067 067 059 021 236 254 106 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 083 253 209 018 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 022 233 255 083 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 129 254 238 044 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 059 249 254 062 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 133 254 187 005 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 009 205 248 058 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 126 254 182 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 075 251 240 057 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 019 221 254 166 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 003 203 254 219 035 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 038 254 254 077 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 031 224 254 115 001 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 133 254 254 052 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 061 242 254 254 052 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 121 254 254 219 040 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 121 254 207 018 000 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 \n",
      "テストラベル(y_test) = 7\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from PIL import Image\n",
    "\n",
    "train_no = 0\n",
    "\n",
    "print('訓練画像')\n",
    "for xs in x_train[train_no] :\n",
    "    for x in xs :\n",
    "        sys.stdout.write('%03d ' % x)\n",
    "    sys.stdout.write('\\n')\n",
    "      \n",
    "outImg = Image.fromarray(x_train[train_no].reshape((28, 28))).convert('RGB')\n",
    "outImg.save('train.png')\n",
    "      \n",
    "print('訓練ラベル (y_train) = %d' % y_train[train_no])\n",
    "      \n",
    "test_no = 0\n",
    "      \n",
    "print('テスト画像')\n",
    "for xs in x_test[test_no]:\n",
    "    for x in xs:\n",
    "        sys.stdout.write('%03d ' % x)\n",
    "    sys.stdout.write('\\n')\n",
    "      \n",
    "outImg = Image.fromarray(x_test[test_no].reshape((28, 28))).convert('RGB')\n",
    "outImg.save('test.png')\n",
    "\n",
    "print('テストラベル(y_test) = %d' % y_test[test_no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "YYbkWZB1J-tL",
    "outputId": "56cea2d1-307e-41fb-d914-212b8e4cec9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "y_train = ks.utils.to_categorical(y_train, num_classes)\n",
    "y_test = ks.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784, )))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "72z-3bT3KDCd",
    "outputId": "d767754d-bf60-49e0-b91f-a419fb081010"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0180 - acc: 0.9951 - val_loss: 0.1367 - val_acc: 0.9808\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 0.0187 - acc: 0.9954 - val_loss: 0.1253 - val_acc: 0.9830\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0167 - acc: 0.9956 - val_loss: 0.1264 - val_acc: 0.9828\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0181 - acc: 0.9959 - val_loss: 0.1183 - val_acc: 0.9846\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0180 - acc: 0.9957 - val_loss: 0.1183 - val_acc: 0.9830\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0178 - acc: 0.9963 - val_loss: 0.1242 - val_acc: 0.9827\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.0169 - acc: 0.9963 - val_loss: 0.1249 - val_acc: 0.9829\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.0147 - acc: 0.9962 - val_loss: 0.1248 - val_acc: 0.9843\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.0145 - acc: 0.9965 - val_loss: 0.1367 - val_acc: 0.9824\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.0162 - acc: 0.9963 - val_loss: 0.1395 - val_acc: 0.9829\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 0.0144 - acc: 0.9966 - val_loss: 0.1497 - val_acc: 0.9809\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.0146 - acc: 0.9969 - val_loss: 0.1301 - val_acc: 0.9832\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 0.0145 - acc: 0.9970 - val_loss: 0.1453 - val_acc: 0.9829\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.0149 - acc: 0.9969 - val_loss: 0.1426 - val_acc: 0.9834\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.0160 - acc: 0.9965 - val_loss: 0.1231 - val_acc: 0.9848\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.0141 - acc: 0.9968 - val_loss: 0.1358 - val_acc: 0.9838\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.0154 - acc: 0.9967 - val_loss: 0.1393 - val_acc: 0.9831\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.0154 - acc: 0.9965 - val_loss: 0.1260 - val_acc: 0.9833\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.0117 - acc: 0.9973 - val_loss: 0.1384 - val_acc: 0.9839\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0147 - acc: 0.9968 - val_loss: 0.1393 - val_acc: 0.9841\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "kerasのmnistサンプルを読もうなどと試みる③",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
