{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggleのタイタニックの生存者予測をしようなどと試みる⑧-2",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSzChKbJaITW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBJSjLODaWlc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "c52b5c09-b426-4747-ed98-7aedc7b81da6"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import re\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "all = pd.concat([train, test], axis =0).reset_index(drop=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcUvOzdsasHV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "outputId": "8b2e56db-43be-4e99-dd26-1bbfe2733ad5"
      },
      "source": [
        "all[\"Title\"] = all[\"Name\"].apply(lambda x:re.split(\"[,.]\", x)[1])\n",
        "all[\"Cabin\"].fillna(\"U\", inplace=True)\n",
        "all[\"Cabin\"] = all[\"Cabin\"].apply(lambda x:x[0])\n",
        "all.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Name</th>\n",
              "      <th>Parch</th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22.0</td>\n",
              "      <td>U</td>\n",
              "      <td>S</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>Mr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>38.0</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>Mrs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>26.0</td>\n",
              "      <td>U</td>\n",
              "      <td>S</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>Miss</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>35.0</td>\n",
              "      <td>C</td>\n",
              "      <td>S</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>113803</td>\n",
              "      <td>Mrs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>35.0</td>\n",
              "      <td>U</td>\n",
              "      <td>S</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>373450</td>\n",
              "      <td>Mr</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Age Cabin Embarked     Fare  ... SibSp  Survived            Ticket  Title\n",
              "0  22.0     U        S   7.2500  ...     1       0.0         A/5 21171     Mr\n",
              "1  38.0     C        C  71.2833  ...     1       1.0          PC 17599    Mrs\n",
              "2  26.0     U        S   7.9250  ...     0       1.0  STON/O2. 3101282   Miss\n",
              "3  35.0     C        S  53.1000  ...     1       1.0            113803    Mrs\n",
              "4  35.0     U        S   8.0500  ...     0       0.0            373450     Mr\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqvA7nq7a_66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all[\"col_Pclass_Title\"] = all.apply(lambda x:\"{}_{}\".format(x[\"Pclass\"], x[\"Title\"]), axis=1)\n",
        "\n",
        "age_mean = all.groupby(\"col_Pclass_Title\")[\"Age\"].mean()\n",
        "\n",
        "col_mask = all[\"Age\"].map(lambda x: np.isnan(x))\n",
        "\n",
        "all.loc[col_mask, \"Age\"] = all.loc[col_mask, \"col_Pclass_Title\"].map(age_mean)\n",
        "\n",
        "all.drop(columns=\"col_Pclass_Title\", inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LRrWWdfbJ5S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "7a8865e9-567d-49f2-e1a6-b276ccbac96f"
      },
      "source": [
        "all.groupby([\"Title\"], as_index=False)[\"Age\"].mean()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Capt</td>\n",
              "      <td>70.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Col</td>\n",
              "      <td>54.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Don</td>\n",
              "      <td>40.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Dona</td>\n",
              "      <td>39.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Dr</td>\n",
              "      <td>43.825000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Jonkheer</td>\n",
              "      <td>38.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Lady</td>\n",
              "      <td>48.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Major</td>\n",
              "      <td>48.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Master</td>\n",
              "      <td>5.562295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Miss</td>\n",
              "      <td>21.001247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Mlle</td>\n",
              "      <td>24.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Mme</td>\n",
              "      <td>24.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Mr</td>\n",
              "      <td>31.875231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Mrs</td>\n",
              "      <td>36.912856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Ms</td>\n",
              "      <td>28.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Rev</td>\n",
              "      <td>41.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Sir</td>\n",
              "      <td>49.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>the Countess</td>\n",
              "      <td>33.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Title        Age\n",
              "0            Capt  70.000000\n",
              "1             Col  54.000000\n",
              "2             Don  40.000000\n",
              "3            Dona  39.000000\n",
              "4              Dr  43.825000\n",
              "5        Jonkheer  38.000000\n",
              "6            Lady  48.000000\n",
              "7           Major  48.500000\n",
              "8          Master   5.562295\n",
              "9            Miss  21.001247\n",
              "10           Mlle  24.000000\n",
              "11            Mme  24.000000\n",
              "12             Mr  31.875231\n",
              "13            Mrs  36.912856\n",
              "14             Ms  28.000000\n",
              "15            Rev  41.250000\n",
              "16            Sir  49.000000\n",
              "17   the Countess  33.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjCvGs3ubNgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all[\"Age\"].fillna(28, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJlY6Gy3bcqp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "0368e0ef-4b5f-444f-e3bb-719198d1a5b1"
      },
      "source": [
        "all.groupby([\"Pclass\"], as_index=False)[\"Fare\"].mean()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Fare</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>87.508992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>21.179196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>13.302889</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pclass       Fare\n",
              "0       1  87.508992\n",
              "1       2  21.179196\n",
              "2       3  13.302889"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEdTEeb9bg7y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "50ae3cba-b7fe-468d-ea9b-6108a5371bc2"
      },
      "source": [
        "all.Fare[1043] = 13.302889"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OSVuVGcbk0g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "outputId": "0a602ab9-0cf4-4d4a-8123-4252904066d8"
      },
      "source": [
        "all[\"Embarked\"] = all[\"Embarked\"].fillna(\"S\")\n",
        "all.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Name</th>\n",
              "      <th>Parch</th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22.0</td>\n",
              "      <td>U</td>\n",
              "      <td>S</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>Mr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>38.0</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>Mrs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>26.0</td>\n",
              "      <td>U</td>\n",
              "      <td>S</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>Miss</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>35.0</td>\n",
              "      <td>C</td>\n",
              "      <td>S</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>113803</td>\n",
              "      <td>Mrs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>35.0</td>\n",
              "      <td>U</td>\n",
              "      <td>S</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>373450</td>\n",
              "      <td>Mr</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Age Cabin Embarked     Fare  ... SibSp  Survived            Ticket  Title\n",
              "0  22.0     U        S   7.2500  ...     1       0.0         A/5 21171     Mr\n",
              "1  38.0     C        C  71.2833  ...     1       1.0          PC 17599    Mrs\n",
              "2  26.0     U        S   7.9250  ...     0       1.0  STON/O2. 3101282   Miss\n",
              "3  35.0     C        S  53.1000  ...     1       1.0            113803    Mrs\n",
              "4  35.0     U        S   8.0500  ...     0       0.0            373450     Mr\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVzsKPtEbre_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4606229-9890-4b5c-8b86-6b9b968c2230"
      },
      "source": [
        "cols = [\"Sex\",\"Embarked\",\"Cabin\",\"Title\"]\n",
        "all_to_cat = pd.get_dummies(all, columns=cols)\n",
        "print(all_to_cat.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1309, 41)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIa27AA0bxFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_counts = all['Title'].value_counts()\n",
        "Title_to_drop = data_counts[data_counts < 3].index.tolist()\n",
        "\n",
        "to_drop_list = []\n",
        "target_col = \"Title\"\n",
        "\n",
        "for each_data in Title_to_drop:\n",
        "    col_name = \"{}_{}\".format(target_col, each_data)\n",
        "    to_drop_list.append(col_name)\n",
        "    \n",
        "all_to_cat.drop(columns=to_drop_list, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPX4_ztlbzSF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35cdeab9-8933-441f-ffc9-b09a3632ccfb"
      },
      "source": [
        "to_drop = [\"PassengerId\", \"Name\", \"Ticket\"]\n",
        "\n",
        "all_to_cat.drop(columns=to_drop, inplace=True)\n",
        "print(all_to_cat.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1309, 27)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXzal700b9vv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "23dcfa93-19c2-4308-8d9e-22cd1cbcad67"
      },
      "source": [
        "all_to_cat[\"Pclass\"] /= 3\n",
        "all_to_cat[\"Age\"] /= 80\n",
        "all_to_cat[\"SibSp\"] /= 8\n",
        "all_to_cat[\"Parch\"] /= 9\n",
        "all_to_cat[\"Fare\"] /= 512.3292\n",
        "\n",
        "all_to_cat.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Sex_female</th>\n",
              "      <th>Sex_male</th>\n",
              "      <th>Embarked_C</th>\n",
              "      <th>Embarked_Q</th>\n",
              "      <th>Embarked_S</th>\n",
              "      <th>Cabin_A</th>\n",
              "      <th>Cabin_B</th>\n",
              "      <th>Cabin_C</th>\n",
              "      <th>Cabin_D</th>\n",
              "      <th>Cabin_E</th>\n",
              "      <th>Cabin_F</th>\n",
              "      <th>Cabin_G</th>\n",
              "      <th>Cabin_T</th>\n",
              "      <th>Cabin_U</th>\n",
              "      <th>Title_ Col</th>\n",
              "      <th>Title_ Dr</th>\n",
              "      <th>Title_ Master</th>\n",
              "      <th>Title_ Miss</th>\n",
              "      <th>Title_ Mr</th>\n",
              "      <th>Title_ Mrs</th>\n",
              "      <th>Title_ Rev</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.014151</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.4750</td>\n",
              "      <td>0.139136</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.3250</td>\n",
              "      <td>0.015469</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.4375</td>\n",
              "      <td>0.103644</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.4375</td>\n",
              "      <td>0.015713</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Age      Fare  Parch  ...  Title_ Mr  Title_ Mrs  Title_ Rev\n",
              "0  0.2750  0.014151    0.0  ...          1           0           0\n",
              "1  0.4750  0.139136    0.0  ...          0           1           0\n",
              "2  0.3250  0.015469    0.0  ...          0           0           0\n",
              "3  0.4375  0.103644    0.0  ...          0           1           0\n",
              "4  0.4375  0.015713    0.0  ...          1           0           0\n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pno-mMt8cLbM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "1b85ea1d-cb01-44e2-e545-e553705a9910"
      },
      "source": [
        "train_to_cat = all_to_cat.iloc[0:891, :]\n",
        "train_to_cat.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Sex_female</th>\n",
              "      <th>Sex_male</th>\n",
              "      <th>Embarked_C</th>\n",
              "      <th>Embarked_Q</th>\n",
              "      <th>Embarked_S</th>\n",
              "      <th>Cabin_A</th>\n",
              "      <th>Cabin_B</th>\n",
              "      <th>Cabin_C</th>\n",
              "      <th>Cabin_D</th>\n",
              "      <th>Cabin_E</th>\n",
              "      <th>Cabin_F</th>\n",
              "      <th>Cabin_G</th>\n",
              "      <th>Cabin_T</th>\n",
              "      <th>Cabin_U</th>\n",
              "      <th>Title_ Col</th>\n",
              "      <th>Title_ Dr</th>\n",
              "      <th>Title_ Master</th>\n",
              "      <th>Title_ Miss</th>\n",
              "      <th>Title_ Mr</th>\n",
              "      <th>Title_ Mrs</th>\n",
              "      <th>Title_ Rev</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.014151</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.4750</td>\n",
              "      <td>0.139136</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.3250</td>\n",
              "      <td>0.015469</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.4375</td>\n",
              "      <td>0.103644</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.4375</td>\n",
              "      <td>0.015713</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Age      Fare  Parch  ...  Title_ Mr  Title_ Mrs  Title_ Rev\n",
              "0  0.2750  0.014151    0.0  ...          1           0           0\n",
              "1  0.4750  0.139136    0.0  ...          0           1           0\n",
              "2  0.3250  0.015469    0.0  ...          0           0           0\n",
              "3  0.4375  0.103644    0.0  ...          0           1           0\n",
              "4  0.4375  0.015713    0.0  ...          1           0           0\n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02PLP7IxccAl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "8e791566-32ba-4c6b-f361-6f29ec562ff8"
      },
      "source": [
        "train_train = train_to_cat.drop([\"Survived\"], axis=1)\n",
        "train_train.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Sex_female</th>\n",
              "      <th>Sex_male</th>\n",
              "      <th>Embarked_C</th>\n",
              "      <th>Embarked_Q</th>\n",
              "      <th>Embarked_S</th>\n",
              "      <th>Cabin_A</th>\n",
              "      <th>Cabin_B</th>\n",
              "      <th>Cabin_C</th>\n",
              "      <th>Cabin_D</th>\n",
              "      <th>Cabin_E</th>\n",
              "      <th>Cabin_F</th>\n",
              "      <th>Cabin_G</th>\n",
              "      <th>Cabin_T</th>\n",
              "      <th>Cabin_U</th>\n",
              "      <th>Title_ Col</th>\n",
              "      <th>Title_ Dr</th>\n",
              "      <th>Title_ Master</th>\n",
              "      <th>Title_ Miss</th>\n",
              "      <th>Title_ Mr</th>\n",
              "      <th>Title_ Mrs</th>\n",
              "      <th>Title_ Rev</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.014151</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.4750</td>\n",
              "      <td>0.139136</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.3250</td>\n",
              "      <td>0.015469</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.4375</td>\n",
              "      <td>0.103644</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.4375</td>\n",
              "      <td>0.015713</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Age      Fare  Parch  ...  Title_ Mr  Title_ Mrs  Title_ Rev\n",
              "0  0.2750  0.014151    0.0  ...          1           0           0\n",
              "1  0.4750  0.139136    0.0  ...          0           1           0\n",
              "2  0.3250  0.015469    0.0  ...          0           0           0\n",
              "3  0.4375  0.103644    0.0  ...          0           1           0\n",
              "4  0.4375  0.015713    0.0  ...          1           0           0\n",
              "\n",
              "[5 rows x 26 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCe1a5oEchG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target = train_to_cat[\"Survived\"].values\n",
        "features = train_train.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcF7FsIrcqzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, t_train, t_test = train_test_split(features, target, test_size =0.3, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iREOfTavc-Re",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "046ccafd-564b-4066-a2b1-0aaf13f1aed0"
      },
      "source": [
        "lgb.LGBMClassifier()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
              "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
              "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
              "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
              "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
              "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKH7HcAsdBbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = lgb.Dataset(x_train, label=t_train)\n",
        "eval_data = lgb.Dataset(x_test, label=t_test, reference=train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hBlxAr7dSAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param1 = {\n",
        "    'task': 'train',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'learning_rate': 0.1,\n",
        "    'min_data_in_leaf': 20,\n",
        "    'feature_fraction': 1.0,\n",
        "    'num_leaves': 31,\n",
        "    'max_depth': 7,\n",
        "    'n_estimators': 100,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O8ZTYMEdUow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param2 = {\n",
        "    'task': 'train',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'learning_rate': 0.1,\n",
        "    'min_data_in_leaf': 20,\n",
        "    'feature_fraction': 0.7,\n",
        "    'num_leaves': 100,\n",
        "    'max_depth': 10,\n",
        "    'n_estimators': 60,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASn_rFR6dWda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param3 = {\n",
        "    'task': 'train',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'learning_rate': 0.1,\n",
        "    'min_data_in_leaf': 20,\n",
        "    'feature_fraction': 0.7,\n",
        "    'num_leaves': 200,\n",
        "    'max_depth': 20,\n",
        "    'n_estimators': 60,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRGSjOYOdX6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param4 = {\n",
        "    'task': 'train',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'learning_rate': 0.1,\n",
        "    'min_data_in_leaf': 20,\n",
        "    'feature_fraction': 0.7,\n",
        "    'num_leaves': 300,\n",
        "    'max_depth': 30,\n",
        "    'n_estimators': 60,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTMdoZbmdZXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param5 = {\n",
        "    'task': 'train',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'learning_rate': 0.05,\n",
        "    'min_data_in_leaf': 20,\n",
        "    'feature_fraction': 0.7,\n",
        "    'num_leaves': 400,\n",
        "    'max_depth': 40,\n",
        "    'n_estimators': 50,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjF1Ntp1da7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param6 = {\n",
        "    'task': 'train',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'learning_rate': 0.05,\n",
        "    'min_data_in_leaf': 20,\n",
        "    'feature_fraction': 0.7,\n",
        "    'num_leaves': 500,\n",
        "    'max_depth': 50,\n",
        "    'n_estimators': 50,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxKQUwtEdcvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param7 = {\n",
        "    'task': 'train',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'learning_rate': 0.05,\n",
        "    'min_data_in_leaf': 20,\n",
        "    'feature_fraction': 0.7,\n",
        "    'num_leaves': 600,\n",
        "    'max_depth': 60,\n",
        "    'n_estimators': 50,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdx4aMo9de8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param8 = {\n",
        "    'task': 'train',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'learning_rate': 0.05,\n",
        "    'min_data_in_leaf': 20,\n",
        "    'feature_fraction': 0.7,\n",
        "    'num_leaves': 700,\n",
        "    'max_depth': 70,\n",
        "    'n_estimators': 50,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UYIDiOidgpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param9 = {\n",
        "    'task': 'train',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'learning_rate': 0.03,\n",
        "    'min_data_in_leaf': 20,\n",
        "    'feature_fraction': 0.7,\n",
        "    'num_leaves': 800,\n",
        "    'max_depth': 80,\n",
        "    'n_estimators': 40,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnEo4SVodip8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param10 = {\n",
        "    'task': 'train',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'learning_rate': 0.03,\n",
        "    'min_data_in_leaf': 20,\n",
        "    'feature_fraction': 0.7,\n",
        "    'num_leaves': 900,\n",
        "    'max_depth': 90,\n",
        "    'n_estimators': 40,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5sd5D6OkIdX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "68905490-ea39-4343-b4a9-8ae92a60f9f8"
      },
      "source": [
        "gbm1 = lgb.train(\n",
        "    param1,\n",
        "    train_data,\n",
        "    valid_sets=eval_data,\n",
        "    num_boost_round=500,\n",
        "    early_stopping_rounds=100,\n",
        "    verbose_eval=20,\n",
        ")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[20]\tvalid_0's binary_logloss: 0.404132\n",
            "[40]\tvalid_0's binary_logloss: 0.38445\n",
            "[60]\tvalid_0's binary_logloss: 0.396426\n",
            "[80]\tvalid_0's binary_logloss: 0.410016\n",
            "[100]\tvalid_0's binary_logloss: 0.417354\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[38]\tvalid_0's binary_logloss: 0.383438\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6Y_8NACkLvD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "ac4ccd7d-77dc-4760-fce6-388e68f4ed05"
      },
      "source": [
        "gbm2 = lgb.train(\n",
        "    param2,\n",
        "    train_data,\n",
        "    valid_sets=eval_data,\n",
        "    num_boost_round=500,\n",
        "    early_stopping_rounds=100,\n",
        "    verbose_eval=20,\n",
        ")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[20]\tvalid_0's binary_logloss: 0.403104\n",
            "[40]\tvalid_0's binary_logloss: 0.384978\n",
            "[60]\tvalid_0's binary_logloss: 0.39452\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[34]\tvalid_0's binary_logloss: 0.382935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT3rNiQAkN9q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "0ba506db-d93a-4a18-afde-f8b9a64c0db8"
      },
      "source": [
        "gbm3 = lgb.train(\n",
        "    param3,\n",
        "    train_data,\n",
        "    valid_sets=eval_data,\n",
        "    num_boost_round=500,\n",
        "    early_stopping_rounds=100,\n",
        "    verbose_eval=20,\n",
        ")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[20]\tvalid_0's binary_logloss: 0.402804\n",
            "[40]\tvalid_0's binary_logloss: 0.387252\n",
            "[60]\tvalid_0's binary_logloss: 0.395064\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[41]\tvalid_0's binary_logloss: 0.38498\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3KLvVNJkQNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "b13c3103-ea15-4bfd-fb75-e75adb6e09b6"
      },
      "source": [
        "gbm4 = lgb.train(\n",
        "    param4,\n",
        "    train_data,\n",
        "    valid_sets=eval_data,\n",
        "    num_boost_round=500,\n",
        "    early_stopping_rounds=100,\n",
        "    verbose_eval=20,\n",
        ")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[20]\tvalid_0's binary_logloss: 0.402804\n",
            "[40]\tvalid_0's binary_logloss: 0.387252\n",
            "[60]\tvalid_0's binary_logloss: 0.395064\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[41]\tvalid_0's binary_logloss: 0.38498\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x64Mm8B1kSW9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "0dde24cd-eecd-455e-dfb8-fb2aa02ef436"
      },
      "source": [
        "gbm5 = lgb.train(\n",
        "    param5,\n",
        "    train_data,\n",
        "    valid_sets=eval_data,\n",
        "    num_boost_round=500,\n",
        "    early_stopping_rounds=100,\n",
        "    verbose_eval=20,\n",
        ")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[20]\tvalid_0's binary_logloss: 0.457919\n",
            "[40]\tvalid_0's binary_logloss: 0.406726\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[50]\tvalid_0's binary_logloss: 0.398795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxOSI96ZkUi2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "dd8947d1-5019-4e9a-fed0-861cf2969437"
      },
      "source": [
        "gbm6 = lgb.train(\n",
        "    param6,\n",
        "    train_data,\n",
        "    valid_sets=eval_data,\n",
        "    num_boost_round=500,\n",
        "    early_stopping_rounds=100,\n",
        "    verbose_eval=20,\n",
        ")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[20]\tvalid_0's binary_logloss: 0.457919\n",
            "[40]\tvalid_0's binary_logloss: 0.406726\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[50]\tvalid_0's binary_logloss: 0.398795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE_x81tlkWwu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "b2845545-302c-451a-c026-a5dd12ed7e78"
      },
      "source": [
        "gbm7 = lgb.train(\n",
        "    param7,\n",
        "    train_data,\n",
        "    valid_sets=eval_data,\n",
        "    num_boost_round=500,\n",
        "    early_stopping_rounds=100,\n",
        "    verbose_eval=20,\n",
        ")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[20]\tvalid_0's binary_logloss: 0.457919\n",
            "[40]\tvalid_0's binary_logloss: 0.406726\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[50]\tvalid_0's binary_logloss: 0.398795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KnoazE-kY6m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "8c0a8c98-fea8-4519-f82a-6109a1915f83"
      },
      "source": [
        "gbm8 = lgb.train(\n",
        "    param8,\n",
        "    train_data,\n",
        "    valid_sets=eval_data,\n",
        "    num_boost_round=500,\n",
        "    early_stopping_rounds=100,\n",
        "    verbose_eval=20,\n",
        ")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[20]\tvalid_0's binary_logloss: 0.457919\n",
            "[40]\tvalid_0's binary_logloss: 0.406726\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[50]\tvalid_0's binary_logloss: 0.398795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qGZKDuoka2V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "44c856d3-1a02-4528-f597-80d936c4c5e2"
      },
      "source": [
        "gbm9 = lgb.train(\n",
        "    param9,\n",
        "    train_data,\n",
        "    valid_sets=eval_data,\n",
        "    num_boost_round=500,\n",
        "    early_stopping_rounds=100,\n",
        "    verbose_eval=20,\n",
        ")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[20]\tvalid_0's binary_logloss: 0.50444\n",
            "[40]\tvalid_0's binary_logloss: 0.444821\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[40]\tvalid_0's binary_logloss: 0.444821\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRUp4su2kc7b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "3eb389d2-572a-4a0a-a6a4-da602cc7c98f"
      },
      "source": [
        "gbm10 = lgb.train(\n",
        "    param10,\n",
        "    train_data,\n",
        "    valid_sets=eval_data,\n",
        "    num_boost_round=500,\n",
        "    early_stopping_rounds=100,\n",
        "    verbose_eval=20,\n",
        ")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[20]\tvalid_0's binary_logloss: 0.50444\n",
            "[40]\tvalid_0's binary_logloss: 0.444821\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[40]\tvalid_0's binary_logloss: 0.444821\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mICo5oLSlJYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_to_cat = all_to_cat.iloc[891:, :]\n",
        "test_test = test_to_cat.drop([\"Survived\"], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwFgiYfhlNlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_features = test_test.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0-5kzUnlSNr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4d6e11a5-e800-42b4-a11c-bcf8177acdee"
      },
      "source": [
        "pred1 = gbm1.predict(test_features)\n",
        "pred2 = gbm2.predict(test_features) \n",
        "pred3 = gbm3.predict(test_features)\n",
        "pred5 = gbm5.predict(test_features)\n",
        "pred9 = gbm9.predict(test_features)\n",
        "print(pred1)\n",
        "print(pred2)\n",
        "print(pred3)\n",
        "print(pred5)\n",
        "print(pred9)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.06703583 0.39529158 0.03376029 0.22169699 0.49608024 0.19207886\n",
            " 0.51433002 0.23122995 0.84743183 0.0326693  0.07400317 0.08006122\n",
            " 0.94572294 0.0252844  0.95374849 0.93118449 0.1402043  0.18990642\n",
            " 0.34337561 0.53932746 0.08191271 0.63173928 0.96881923 0.16497355\n",
            " 0.94060392 0.04645717 0.9686836  0.18990642 0.4606397  0.05824561\n",
            " 0.05523749 0.04408283 0.50820206 0.42118514 0.37024788 0.17556527\n",
            " 0.46787169 0.50841095 0.0732695  0.30412737 0.09755506 0.4132222\n",
            " 0.06501642 0.9020145  0.95089881 0.07838134 0.32363608 0.1416509\n",
            " 0.96480705 0.62481248 0.21964412 0.24168989 0.88340403 0.63634222\n",
            " 0.45911497 0.07850494 0.06641039 0.28534979 0.0664148  0.97740239\n",
            " 0.0731795  0.34465092 0.12150013 0.81080074 0.75175614 0.93404994\n",
            " 0.82936402 0.05767566 0.59857163 0.86298684 0.80180414 0.06074623\n",
            " 0.31750934 0.57405517 0.94992988 0.59106969 0.09873853 0.75560244\n",
            " 0.19606148 0.80180414 0.87287174 0.15992838 0.09775516 0.07400317\n",
            " 0.32671076 0.15192498 0.63677702 0.46787169 0.53633774 0.89181971\n",
            " 0.51697792 0.15558138 0.90675094 0.09873853 0.25730495 0.12913666\n",
            " 0.91932376 0.23582919 0.60786387 0.1772609  0.93665438 0.1291507\n",
            " 0.1416509  0.25836734 0.68083567 0.12867046 0.07716084 0.1416509\n",
            " 0.11721342 0.23530637 0.1188682  0.82387908 0.96997887 0.81080074\n",
            " 0.90424377 0.18273607 0.10169212 0.69188541 0.42775973 0.87622502\n",
            " 0.89409239 0.12837207 0.95407916 0.13111568 0.1416509  0.52536428\n",
            " 0.12913666 0.75016171 0.14733682 0.04912897 0.3369963  0.37000235\n",
            " 0.38089161 0.10248037 0.06501642 0.07358193 0.23988262 0.17092876\n",
            " 0.46183961 0.05450273 0.05272541 0.9743891  0.13940791 0.14187078\n",
            " 0.42152755 0.11252656 0.16173625 0.04912897 0.42152755 0.46372511\n",
            " 0.95816279 0.11240321 0.02315861 0.47248779 0.05266651 0.12749811\n",
            " 0.93822653 0.58367371 0.4132222  0.48085351 0.81080074 0.87287174\n",
            " 0.8756593  0.06878266 0.33348051 0.29491738 0.1547321  0.03097902\n",
            " 0.88469666 0.50841095 0.08236998 0.25964073 0.04281229 0.12167175\n",
            " 0.06185425 0.9074425  0.95432388 0.14675468 0.97652843 0.9511961\n",
            " 0.19606148 0.53419814 0.94347537 0.1416509  0.92813127 0.07813154\n",
            " 0.91778333 0.04696561 0.05805683 0.13889658 0.09494104 0.15512123\n",
            " 0.69270013 0.03376029 0.7800907  0.07104541 0.91195862 0.58391663\n",
            " 0.04641186 0.51847215 0.81929243 0.81815829 0.58855845 0.914446\n",
            " 0.06812128 0.17404396 0.55208172 0.04641186 0.94471595 0.05536628\n",
            " 0.34465092 0.06878266 0.18511731 0.76418221 0.13548195 0.10714157\n",
            " 0.82936402 0.08526244 0.92695901 0.09873853 0.90544789 0.05393745\n",
            " 0.91462313 0.14061782 0.88671095 0.72809696 0.12913666 0.82387908\n",
            " 0.07512494 0.12655388 0.30546268 0.94360613 0.11635959 0.07306177\n",
            " 0.40037808 0.11573172 0.13373625 0.18990642 0.84930212 0.94770322\n",
            " 0.87677836 0.90508821 0.29572041 0.07400317 0.64902421 0.2227346\n",
            " 0.94230075 0.06248007 0.87622502 0.56576041 0.96037263 0.11329069\n",
            " 0.52275249 0.05000745 0.18578274 0.08236998 0.1416509  0.12440231\n",
            " 0.85231511 0.14061782 0.04869501 0.08058091 0.95046195 0.62198055\n",
            " 0.26602313 0.07400317 0.03943438 0.08236998 0.46787169 0.11171704\n",
            " 0.2081399  0.1416509  0.88610694 0.81412945 0.10568154 0.89799312\n",
            " 0.13370708 0.07065938 0.13041649 0.04641186 0.47587563 0.82686452\n",
            " 0.82387908 0.70968083 0.57701418 0.02648164 0.08236998 0.39094561\n",
            " 0.32284853 0.09873853 0.11649073 0.4813864  0.12167175 0.45236963\n",
            " 0.11215381 0.13543867 0.95950706 0.05824561 0.17847069 0.23224291\n",
            " 0.29896439 0.64943622 0.06235511 0.05179126 0.82387908 0.85769791\n",
            " 0.31192351 0.83559163 0.156427   0.33102976 0.09148035 0.18990642\n",
            " 0.08236998 0.4242853  0.91987875 0.80219368 0.17455507 0.08960861\n",
            " 0.20466118 0.04408283 0.25836734 0.3665072  0.34041336 0.58502901\n",
            " 0.9644085  0.12354359 0.93887473 0.1355776  0.13928506 0.04469926\n",
            " 0.95534308 0.52226589 0.10568154 0.7481519  0.14007893 0.39483125\n",
            " 0.38173455 0.03713376 0.13521692 0.7755044  0.08233653 0.16513539\n",
            " 0.07968506 0.9562811  0.73878588 0.58153109 0.17092876 0.59706317\n",
            " 0.03619772 0.93129426 0.95399982 0.06812128 0.15388815 0.10236194\n",
            " 0.57701418 0.22157236 0.91871411 0.07400317 0.1416509  0.42633177\n",
            " 0.05573494 0.95047667 0.95325628 0.22169699 0.97132599 0.28863735\n",
            " 0.15192498 0.3484788  0.95399982 0.26602313 0.05355985 0.97803777\n",
            " 0.07139829 0.13227666 0.96639096 0.93609861 0.42317324 0.05096866\n",
            " 0.05602522 0.12751824 0.1416509  0.1290946  0.5532555  0.57684708\n",
            " 0.34993125 0.90270141 0.12913666 0.03626128 0.12967797 0.15976528\n",
            " 0.3306067  0.97344906 0.70772039 0.07841668 0.0600244  0.87356442\n",
            " 0.10865676 0.96748743 0.12913666 0.13219101 0.93822653 0.04869501\n",
            " 0.96835139 0.16275775 0.36084983 0.34361741 0.0487833  0.27836944\n",
            " 0.81080074 0.62198055 0.82387908 0.96020353 0.39707904 0.09873853\n",
            " 0.94228826 0.0279399  0.09873853 0.87287174]\n",
            "[0.07071961 0.25102206 0.04407743 0.26232647 0.56575117 0.19347998\n",
            " 0.55498034 0.2510494  0.76099997 0.0342193  0.06174512 0.11238056\n",
            " 0.95688895 0.02775441 0.93449741 0.88588543 0.0989722  0.2206375\n",
            " 0.38381143 0.41955186 0.10077262 0.55745507 0.94667586 0.2681974\n",
            " 0.93264391 0.04550037 0.96945414 0.22310167 0.48538558 0.06088304\n",
            " 0.04985783 0.03664021 0.57405801 0.50737389 0.45583727 0.16783237\n",
            " 0.50590686 0.54376666 0.09441438 0.21592254 0.10842428 0.43480904\n",
            " 0.09165831 0.84402876 0.94543416 0.09123786 0.38058435 0.11897058\n",
            " 0.9554568  0.52096173 0.38221719 0.1735143  0.88513849 0.64803388\n",
            " 0.46432486 0.12472783 0.05275004 0.26332257 0.07610058 0.96986333\n",
            " 0.09308908 0.38119544 0.10918505 0.85735923 0.70614095 0.93692287\n",
            " 0.86470346 0.09936476 0.54781042 0.81383064 0.84798812 0.06748992\n",
            " 0.33512925 0.58898414 0.94166121 0.55191043 0.08034437 0.7618468\n",
            " 0.20017778 0.84798812 0.80134805 0.15352117 0.13954295 0.06174512\n",
            " 0.36555628 0.1133385  0.75951307 0.55240735 0.61744937 0.85117742\n",
            " 0.45708422 0.15251957 0.92344459 0.08034437 0.22337132 0.19288099\n",
            " 0.87180402 0.24126591 0.62661197 0.10842604 0.92349771 0.117489\n",
            " 0.11897058 0.3215412  0.69234833 0.11197199 0.11205768 0.11897058\n",
            " 0.14033156 0.3077098  0.10613225 0.82677244 0.96121043 0.83337913\n",
            " 0.87787312 0.1255023  0.10294543 0.62499312 0.43190135 0.87052646\n",
            " 0.89582259 0.1094325  0.9540816  0.14044066 0.11897058 0.56888511\n",
            " 0.18563246 0.76228096 0.15828238 0.06119448 0.3586375  0.32558288\n",
            " 0.43131989 0.08075466 0.07951586 0.11549362 0.35804691 0.1371629\n",
            " 0.50946186 0.06650359 0.07644224 0.97087551 0.16192858 0.11322888\n",
            " 0.42009046 0.1420239  0.17128218 0.06109872 0.35348205 0.4352379\n",
            " 0.91910572 0.09794337 0.03603789 0.43737882 0.09206258 0.12708082\n",
            " 0.91949413 0.60112261 0.48931505 0.40530578 0.82677244 0.80134805\n",
            " 0.77564134 0.07579431 0.28267121 0.45106911 0.20687447 0.03450315\n",
            " 0.88901204 0.56428915 0.08320264 0.35164146 0.04084504 0.12784227\n",
            " 0.0692127  0.92491631 0.92546709 0.16741054 0.95507629 0.92498091\n",
            " 0.20017778 0.4724266  0.96802688 0.11897058 0.94110863 0.07075216\n",
            " 0.90356905 0.06118349 0.07281394 0.11938547 0.07268692 0.12763783\n",
            " 0.5615708  0.04623954 0.80650043 0.08007247 0.85623157 0.54539993\n",
            " 0.05007832 0.49521551 0.82149364 0.71021001 0.58634755 0.9201804\n",
            " 0.06322179 0.14531858 0.58331955 0.05895197 0.9497688  0.06330684\n",
            " 0.3662511  0.07579431 0.11338892 0.73120488 0.13219922 0.16071211\n",
            " 0.86470346 0.08665401 0.89000872 0.08034437 0.85655191 0.06226427\n",
            " 0.8765901  0.18869623 0.86120616 0.60766238 0.15894025 0.82677244\n",
            " 0.05964913 0.08269357 0.2181793  0.92225925 0.10162116 0.06113234\n",
            " 0.37205531 0.14978307 0.17411592 0.22435094 0.84257344 0.90948589\n",
            " 0.86120616 0.85389261 0.33371403 0.06174512 0.68548619 0.24035127\n",
            " 0.87346238 0.06767578 0.87052646 0.50765427 0.93240264 0.15606955\n",
            " 0.51347909 0.08823633 0.11668734 0.08320264 0.11897058 0.10944788\n",
            " 0.81694305 0.16961265 0.03776121 0.11275331 0.92602842 0.56339446\n",
            " 0.28965856 0.06174512 0.06371558 0.08320264 0.50590686 0.11294995\n",
            " 0.25764164 0.11897058 0.88599228 0.80601219 0.13313711 0.84442922\n",
            " 0.13407703 0.0584587  0.10344268 0.05978801 0.51698131 0.73687949\n",
            " 0.82677244 0.61904058 0.55005773 0.03024196 0.08875662 0.42541653\n",
            " 0.27930338 0.08034437 0.11434559 0.51058817 0.12784227 0.46662865\n",
            " 0.07729539 0.1324128  0.95721625 0.06088304 0.21465937 0.24173891\n",
            " 0.34409385 0.5178873  0.06191682 0.09629142 0.82677244 0.86713773\n",
            " 0.38944261 0.76528315 0.14929305 0.24618547 0.11294995 0.22310167\n",
            " 0.08320264 0.51497057 0.89082084 0.82677244 0.18250267 0.09975518\n",
            " 0.24522652 0.03997874 0.3215412  0.33717    0.35045575 0.49967786\n",
            " 0.95454467 0.09422884 0.95389991 0.20156616 0.12775615 0.05886482\n",
            " 0.89779533 0.49445771 0.13313711 0.73148605 0.13845288 0.39316976\n",
            " 0.423156   0.07688765 0.07056639 0.79781644 0.09599582 0.20422124\n",
            " 0.12680336 0.94533963 0.68518677 0.52685548 0.1371629  0.48284724\n",
            " 0.04770463 0.88519735 0.94216145 0.06322179 0.11338892 0.10564002\n",
            " 0.53837207 0.30740902 0.89389549 0.07015723 0.11897058 0.4606127\n",
            " 0.07727435 0.9532036  0.92015543 0.26232647 0.96694443 0.3333092\n",
            " 0.10677511 0.45725999 0.93879575 0.28067475 0.05921009 0.96285255\n",
            " 0.13580923 0.14221184 0.93828892 0.93382836 0.43738551 0.06386374\n",
            " 0.07237491 0.1334912  0.11897058 0.14820703 0.47138871 0.62339936\n",
            " 0.38857918 0.89488652 0.17346468 0.05544975 0.13445306 0.15659806\n",
            " 0.39695186 0.94147814 0.64453386 0.08212698 0.06294646 0.92728157\n",
            " 0.11483791 0.95515348 0.16679503 0.15079919 0.91949413 0.03266764\n",
            " 0.96351767 0.15058421 0.42960259 0.40091524 0.04575221 0.31847333\n",
            " 0.82677244 0.5347596  0.82677244 0.94062974 0.36048698 0.08034437\n",
            " 0.92301555 0.03024196 0.08034437 0.82143063]\n",
            "[0.06256771 0.24093056 0.03103251 0.300938   0.62434857 0.17981303\n",
            " 0.50753358 0.31013196 0.77968775 0.02844767 0.05129639 0.0848374\n",
            " 0.97052964 0.02481261 0.93278427 0.93299505 0.09943189 0.22902352\n",
            " 0.43343714 0.45000219 0.10675944 0.50825967 0.95067724 0.21403282\n",
            " 0.94370003 0.03195273 0.96662012 0.22606303 0.51728076 0.06404743\n",
            " 0.04230911 0.03221797 0.59066821 0.51272827 0.41245198 0.13960618\n",
            " 0.51023026 0.54093976 0.05947253 0.29186021 0.08100845 0.46884434\n",
            " 0.09158115 0.8555555  0.95898207 0.07956103 0.38335719 0.13349089\n",
            " 0.94880601 0.56263702 0.35330088 0.15816893 0.89832601 0.65228876\n",
            " 0.4846695  0.07795452 0.04864635 0.28725514 0.05591121 0.97659905\n",
            " 0.08093981 0.38707568 0.08378658 0.88474543 0.70651973 0.94171412\n",
            " 0.90515371 0.08545963 0.53930077 0.77092817 0.8725954  0.06531917\n",
            " 0.30807678 0.61549031 0.92869226 0.45955242 0.07151303 0.70866701\n",
            " 0.2130439  0.8725954  0.83966709 0.1244443  0.10095053 0.05129639\n",
            " 0.42021587 0.08909322 0.78076031 0.55668011 0.64432488 0.88876929\n",
            " 0.51513363 0.17901946 0.90142474 0.07151303 0.17557944 0.1709469\n",
            " 0.84746963 0.22773535 0.67844984 0.09280749 0.93916901 0.1335383\n",
            " 0.13349089 0.39048941 0.76630751 0.07494319 0.10435754 0.13349089\n",
            " 0.1210323  0.27082789 0.06038698 0.82352732 0.97448186 0.83662355\n",
            " 0.86409416 0.09124201 0.09595046 0.68175509 0.35816851 0.87471175\n",
            " 0.89222995 0.123326   0.93163821 0.16411612 0.13349089 0.55736163\n",
            " 0.21540565 0.77601433 0.15044532 0.04772433 0.37865541 0.36549555\n",
            " 0.44558617 0.08613466 0.07751168 0.06689232 0.40220893 0.16118043\n",
            " 0.47784234 0.04510507 0.05244457 0.97008165 0.16599336 0.09655796\n",
            " 0.44919446 0.1159407  0.15030883 0.06010738 0.38024797 0.37130934\n",
            " 0.93993395 0.08370532 0.0321533  0.44724043 0.05476418 0.10885084\n",
            " 0.92125847 0.5706881  0.52408972 0.40146563 0.82352732 0.83966709\n",
            " 0.77182556 0.07982637 0.27009485 0.47596035 0.13787249 0.02798548\n",
            " 0.90164803 0.56701181 0.09111066 0.39379452 0.02830899 0.13447557\n",
            " 0.06693323 0.92823585 0.93235993 0.22995633 0.96052499 0.94891516\n",
            " 0.2130439  0.61289237 0.96634062 0.13349089 0.95837725 0.06407902\n",
            " 0.91505201 0.05035248 0.05007569 0.12067522 0.06355046 0.08541825\n",
            " 0.48672255 0.03435498 0.85339177 0.07327938 0.89641345 0.47078108\n",
            " 0.0321612  0.54597669 0.80442054 0.75896316 0.55179288 0.92226537\n",
            " 0.04398036 0.09389572 0.55943127 0.03829047 0.95213115 0.04537567\n",
            " 0.31239625 0.07982637 0.08108351 0.71429209 0.09811604 0.14164834\n",
            " 0.90515371 0.06105364 0.90216407 0.07151303 0.84396189 0.06595832\n",
            " 0.87918103 0.23263194 0.89085082 0.63460665 0.15337618 0.82352732\n",
            " 0.03791678 0.07802838 0.17145445 0.93227118 0.09922982 0.05245308\n",
            " 0.33337492 0.14252943 0.15954647 0.18598306 0.85583728 0.9374842\n",
            " 0.88104711 0.86864447 0.44880313 0.05129639 0.69919012 0.20657699\n",
            " 0.90037876 0.06160692 0.87471175 0.54290336 0.94708971 0.17866518\n",
            " 0.53719673 0.05188714 0.13819892 0.09111066 0.13349089 0.09550061\n",
            " 0.84526545 0.18710986 0.03212614 0.10152256 0.92524832 0.61609344\n",
            " 0.30845868 0.05129639 0.04534702 0.09111066 0.51023026 0.10074519\n",
            " 0.19814536 0.13349089 0.8995404  0.78790384 0.13988764 0.8718891\n",
            " 0.12714565 0.04855883 0.08043603 0.04144505 0.47015196 0.79970213\n",
            " 0.82352732 0.59107457 0.56890055 0.02598132 0.10159769 0.50498033\n",
            " 0.28394592 0.07151303 0.09806774 0.44266129 0.13447557 0.54702234\n",
            " 0.07412381 0.13239901 0.97255116 0.06404743 0.18090439 0.21382961\n",
            " 0.29758344 0.45914312 0.04509658 0.06242962 0.82352732 0.8766499\n",
            " 0.39625662 0.82338703 0.17541879 0.21930161 0.10074519 0.22606303\n",
            " 0.09111066 0.46152729 0.88474902 0.82352732 0.19024563 0.08738366\n",
            " 0.21983161 0.04031485 0.39048941 0.3472278  0.3679566  0.51950766\n",
            " 0.95163183 0.0760499  0.96531455 0.17499327 0.10149248 0.04838033\n",
            " 0.87670821 0.50855996 0.13988764 0.8421789  0.14069974 0.31680592\n",
            " 0.50244989 0.05415111 0.0710539  0.83040609 0.07048352 0.16797473\n",
            " 0.13017247 0.9508935  0.69317334 0.4746636  0.16118043 0.52424483\n",
            " 0.02999939 0.89548449 0.9472559  0.04398036 0.08108351 0.08399747\n",
            " 0.54210042 0.24985842 0.85812876 0.06107283 0.13349089 0.47622322\n",
            " 0.04412392 0.96393279 0.94382673 0.300938   0.97692222 0.31963102\n",
            " 0.08466714 0.46297702 0.94416908 0.29055284 0.04694018 0.98362375\n",
            " 0.10056061 0.14722284 0.96434139 0.94263974 0.44775871 0.05036518\n",
            " 0.0577568  0.10987036 0.13349089 0.13421702 0.45255355 0.70495958\n",
            " 0.4668097  0.87147403 0.15692616 0.04591346 0.14790224 0.1131374\n",
            " 0.44614349 0.94149784 0.67864695 0.06443177 0.03346438 0.88873584\n",
            " 0.11943446 0.96228555 0.17249671 0.11352639 0.92125847 0.0288911\n",
            " 0.95796721 0.09232788 0.47711109 0.30709947 0.0295234  0.26811068\n",
            " 0.82352732 0.58808014 0.82352732 0.95754239 0.36139471 0.07151303\n",
            " 0.93933646 0.0286136  0.07151303 0.86734554]\n",
            "[0.08864437 0.37017707 0.07049223 0.21108008 0.60059382 0.19543377\n",
            " 0.6258383  0.25268188 0.73825197 0.05712815 0.08226138 0.1143624\n",
            " 0.91595993 0.04849344 0.9165002  0.89060609 0.13049872 0.18235822\n",
            " 0.36517147 0.47729988 0.13330962 0.62558891 0.8994077  0.21089738\n",
            " 0.90972339 0.06195703 0.94124988 0.18333293 0.50875724 0.08169817\n",
            " 0.06479708 0.06439765 0.52171968 0.49739583 0.45514196 0.14970275\n",
            " 0.48397996 0.57201954 0.10922613 0.24584502 0.13514992 0.45130303\n",
            " 0.11222076 0.79539899 0.91733893 0.11259961 0.45361497 0.15590625\n",
            " 0.92820865 0.53750587 0.38521565 0.20030993 0.84152279 0.5958734\n",
            " 0.37303714 0.15282864 0.07043224 0.2607587  0.07660349 0.94487427\n",
            " 0.09820104 0.30712896 0.14370527 0.81391568 0.69840409 0.88349757\n",
            " 0.81364325 0.11769406 0.62220257 0.82394157 0.79181551 0.08077532\n",
            " 0.38848171 0.58650584 0.90973712 0.45097934 0.09952518 0.73349783\n",
            " 0.23006415 0.79181551 0.79647115 0.19465145 0.1228322  0.08226138\n",
            " 0.34478349 0.12189347 0.65620541 0.51841862 0.63337301 0.80439661\n",
            " 0.57296683 0.18732186 0.91442162 0.09952518 0.16746318 0.17321644\n",
            " 0.85682118 0.2718151  0.60051269 0.15470146 0.92068954 0.10989354\n",
            " 0.15590625 0.24271437 0.69370757 0.15354527 0.10622742 0.15590625\n",
            " 0.12549127 0.28060733 0.13232786 0.81079684 0.93928263 0.81079684\n",
            " 0.84886339 0.16022682 0.13833235 0.61013843 0.40372952 0.87218634\n",
            " 0.86640315 0.14196538 0.92607096 0.15063957 0.15590625 0.53952418\n",
            " 0.17996314 0.74340603 0.17310946 0.07640832 0.36360124 0.45641123\n",
            " 0.40619118 0.11623437 0.10059073 0.09775865 0.24426498 0.16492604\n",
            " 0.52467598 0.09256831 0.12591735 0.92247797 0.18540704 0.15202424\n",
            " 0.42391336 0.1452795  0.16068456 0.07974808 0.39705903 0.55023019\n",
            " 0.8972283  0.11021701 0.05905446 0.47305149 0.12249684 0.11228078\n",
            " 0.89700098 0.57541507 0.46883465 0.45259678 0.81079684 0.79647115\n",
            " 0.75854923 0.10929001 0.32418552 0.49387097 0.20258393 0.07503349\n",
            " 0.84314835 0.57201954 0.10261836 0.24773599 0.06966765 0.13489642\n",
            " 0.09731469 0.87977911 0.89491443 0.22698506 0.92906832 0.91665779\n",
            " 0.23006415 0.39544465 0.92944196 0.15590625 0.92072574 0.08396647\n",
            " 0.88049085 0.09478356 0.11720465 0.15515869 0.07300495 0.12771292\n",
            " 0.60044125 0.07049223 0.81620543 0.09501487 0.82073824 0.56622786\n",
            " 0.06810736 0.4832662  0.795528   0.67268866 0.63533654 0.88816326\n",
            " 0.08537511 0.15636447 0.6347345  0.06810736 0.90751699 0.07291397\n",
            " 0.28461879 0.10929001 0.15765188 0.73971288 0.20153994 0.17043332\n",
            " 0.81364325 0.10257496 0.8722205  0.09952518 0.80958808 0.08461203\n",
            " 0.85593322 0.1896795  0.81304042 0.61177437 0.16359658 0.81079684\n",
            " 0.08654926 0.11930307 0.24748604 0.8869323  0.12046548 0.07228412\n",
            " 0.41020779 0.15017704 0.20362652 0.17832238 0.77338588 0.88648799\n",
            " 0.81304042 0.84587451 0.29569836 0.08226138 0.66963948 0.26051433\n",
            " 0.88189781 0.09954777 0.87218634 0.52888313 0.88405472 0.16804984\n",
            " 0.48436527 0.08550905 0.15781944 0.10261836 0.15590625 0.15182419\n",
            " 0.79703605 0.18115817 0.05488681 0.10798666 0.88949972 0.5704913\n",
            " 0.25769482 0.08226138 0.08078061 0.10261836 0.48397996 0.12618525\n",
            " 0.2654056  0.15590625 0.88568357 0.78865372 0.14583485 0.83471493\n",
            " 0.15839146 0.07594795 0.1232384  0.07578657 0.53543462 0.68873372\n",
            " 0.81079684 0.66758899 0.57072445 0.05155937 0.10462606 0.38703833\n",
            " 0.26276785 0.09952518 0.14115634 0.6258383  0.13489642 0.53715457\n",
            " 0.11515539 0.10915487 0.921728   0.08169817 0.21564816 0.27684064\n",
            " 0.33534035 0.4722513  0.08761374 0.09014093 0.81079684 0.82330249\n",
            " 0.37244694 0.70332348 0.17705648 0.34285298 0.12618525 0.18333293\n",
            " 0.10261836 0.62331896 0.88457722 0.81079684 0.20362652 0.11121988\n",
            " 0.15233678 0.0683658  0.24271437 0.32070733 0.28626356 0.44372369\n",
            " 0.90911147 0.11245472 0.91764545 0.20414882 0.13830994 0.0882065\n",
            " 0.85191012 0.53380611 0.14583485 0.71168561 0.11422913 0.33460877\n",
            " 0.378473   0.09592036 0.12743682 0.75651595 0.10513525 0.19740385\n",
            " 0.15904291 0.92034374 0.67625233 0.55122897 0.16492604 0.50720732\n",
            " 0.06604287 0.86988496 0.92694059 0.08537511 0.15765188 0.13312562\n",
            " 0.53290077 0.17992543 0.8700341  0.08226138 0.15590625 0.49984469\n",
            " 0.12249684 0.91678465 0.88794064 0.21108008 0.93586119 0.36963378\n",
            " 0.12189347 0.42938966 0.91558841 0.27337569 0.07975147 0.93781881\n",
            " 0.1423362  0.17155557 0.89934275 0.90906415 0.48977724 0.07831478\n",
            " 0.10151991 0.12416505 0.15590625 0.10321157 0.46747513 0.61116356\n",
            " 0.3484705  0.87403134 0.16528385 0.08068039 0.15789221 0.20548484\n",
            " 0.33417159 0.89994705 0.63407071 0.11893805 0.09385087 0.89291015\n",
            " 0.10650627 0.92171481 0.17178577 0.18489961 0.89700098 0.05121398\n",
            " 0.92328579 0.15931    0.472086   0.33169133 0.06895092 0.28390271\n",
            " 0.81079684 0.53756999 0.81079684 0.90364462 0.40539737 0.09952518\n",
            " 0.90830913 0.05155937 0.09952518 0.77623244]\n",
            "[0.1528526  0.41149786 0.15957574 0.22043682 0.53991243 0.19485941\n",
            " 0.5583965  0.29831388 0.64715826 0.15557951 0.15350916 0.15494934\n",
            " 0.79099269 0.13150956 0.77557674 0.7584811  0.16400909 0.22679844\n",
            " 0.39667065 0.50481384 0.23959284 0.50435267 0.76315305 0.27936676\n",
            " 0.7675086  0.13053159 0.80630061 0.22604887 0.44654857 0.18206946\n",
            " 0.13150956 0.17955732 0.4703943  0.4703943  0.45530985 0.2149862\n",
            " 0.50613556 0.52375373 0.17729802 0.26427116 0.1859585  0.36186586\n",
            " 0.18793469 0.67331687 0.7725562  0.17269973 0.36523419 0.20326261\n",
            " 0.78170541 0.49718955 0.45056674 0.22584486 0.71697531 0.51215733\n",
            " 0.33704825 0.30258389 0.13951228 0.32563823 0.16270024 0.80599824\n",
            " 0.18541376 0.29974475 0.20853689 0.69888297 0.65902628 0.74567178\n",
            " 0.70098666 0.18041474 0.50498822 0.7119431  0.65591099 0.15463509\n",
            " 0.41967586 0.50933422 0.76654243 0.41376544 0.16420239 0.69212546\n",
            " 0.2838389  0.65591099 0.6140284  0.31919063 0.16171899 0.15350916\n",
            " 0.30287539 0.19054228 0.56086496 0.52413247 0.5583965  0.64065779\n",
            " 0.53649077 0.21259906 0.77986957 0.16420239 0.22440643 0.22489922\n",
            " 0.73129376 0.30183371 0.54318505 0.17623118 0.77726609 0.17355326\n",
            " 0.20326261 0.25855985 0.62269505 0.1991882  0.16578368 0.20326261\n",
            " 0.19001171 0.34452922 0.18592161 0.69888297 0.81199416 0.69888297\n",
            " 0.72085328 0.21198371 0.21286682 0.55078949 0.41376544 0.75139517\n",
            " 0.74355853 0.1930869  0.78120997 0.20618884 0.20326261 0.52070633\n",
            " 0.22249844 0.64680691 0.20898691 0.16115049 0.31914103 0.54105593\n",
            " 0.41660315 0.19375654 0.17559086 0.16681276 0.26045639 0.21478174\n",
            " 0.48219397 0.18841507 0.19854986 0.76914453 0.32718074 0.19288943\n",
            " 0.35252313 0.25242972 0.21261864 0.16244731 0.3512695  0.49536446\n",
            " 0.75577687 0.17973816 0.13716433 0.45300106 0.26164004 0.1854825\n",
            " 0.75455395 0.49907051 0.36186586 0.43990128 0.69888297 0.6140284\n",
            " 0.64322247 0.18419295 0.37949277 0.46533303 0.24243795 0.18102358\n",
            " 0.74324523 0.53097685 0.18280207 0.26214741 0.15813237 0.21096819\n",
            " 0.18841507 0.75731786 0.74640677 0.33128922 0.80107696 0.77412897\n",
            " 0.2838389  0.40192229 0.80463752 0.20326261 0.78657197 0.15928596\n",
            " 0.74181925 0.19312152 0.19854986 0.20040579 0.13303424 0.18225901\n",
            " 0.49286972 0.15957574 0.64322034 0.15767017 0.65164872 0.54069859\n",
            " 0.16285249 0.45330021 0.68535409 0.52185073 0.51768411 0.77635979\n",
            " 0.16606608 0.20230833 0.5583965  0.16285249 0.78632253 0.15386218\n",
            " 0.29816725 0.18419295 0.22822482 0.68687222 0.27207139 0.21261864\n",
            " 0.70098666 0.23008069 0.73326875 0.16420239 0.71322505 0.16515234\n",
            " 0.73165756 0.22593368 0.72202333 0.54827882 0.21883021 0.69888297\n",
            " 0.16008105 0.17681566 0.23489689 0.74325689 0.1879597  0.14628558\n",
            " 0.4002742  0.21324484 0.32718074 0.22499475 0.70644663 0.74195178\n",
            " 0.72202333 0.72987995 0.29395353 0.15350916 0.51561968 0.32953069\n",
            " 0.73840987 0.17432759 0.75139517 0.50049499 0.77831848 0.21634576\n",
            " 0.44639909 0.17256189 0.17626659 0.18280207 0.20326261 0.18548851\n",
            " 0.69475629 0.22164609 0.1296918  0.17092657 0.7561175  0.52675306\n",
            " 0.29601257 0.15350916 0.1841774  0.18280207 0.50613556 0.19213251\n",
            " 0.2564261  0.20326261 0.74838947 0.68535409 0.21400551 0.72696231\n",
            " 0.20058161 0.14408203 0.22922279 0.16578151 0.48413639 0.53563233\n",
            " 0.69888297 0.62462215 0.51770477 0.1361137  0.18280207 0.44596554\n",
            " 0.35373327 0.16420239 0.21445233 0.5583965  0.21096819 0.5663444\n",
            " 0.17280108 0.17963908 0.79662932 0.18206946 0.28195581 0.29797339\n",
            " 0.32276383 0.42137553 0.16211586 0.17256189 0.69888297 0.73419372\n",
            " 0.44167353 0.53399192 0.32298093 0.40831139 0.19213251 0.22604887\n",
            " 0.18280207 0.5583965  0.73161368 0.69888297 0.32718074 0.18998525\n",
            " 0.19794801 0.1827212  0.25855985 0.36500915 0.32724087 0.35444082\n",
            " 0.73546112 0.18510732 0.80172917 0.22140099 0.22922279 0.175207\n",
            " 0.72987995 0.4501491  0.21400551 0.62292596 0.17963908 0.40345578\n",
            " 0.32234426 0.1816213  0.19530982 0.59159007 0.18480421 0.28835814\n",
            " 0.23691065 0.77379893 0.52615847 0.52707277 0.21478174 0.50481384\n",
            " 0.15705986 0.73981107 0.78359618 0.16606608 0.22822482 0.19576461\n",
            " 0.50035991 0.24601731 0.73269567 0.15350916 0.20326261 0.45048631\n",
            " 0.26164004 0.78434918 0.75089183 0.22043682 0.80203842 0.39656986\n",
            " 0.19054228 0.4131593  0.75724271 0.30060389 0.16378435 0.80378339\n",
            " 0.2452422  0.2079084  0.77223578 0.74152358 0.47140201 0.1663742\n",
            " 0.2366192  0.26164004 0.20326261 0.17428915 0.45480417 0.53584677\n",
            " 0.30887156 0.74568945 0.21663054 0.1721074  0.21329146 0.31483028\n",
            " 0.40531164 0.76995388 0.48541008 0.1633999  0.22507697 0.76273785\n",
            " 0.17999672 0.78296369 0.21825766 0.28087897 0.75455395 0.13040526\n",
            " 0.78255841 0.2285504  0.37785662 0.35414129 0.15982806 0.3233647\n",
            " 0.69888297 0.52210385 0.69888297 0.77713298 0.42436902 0.16420239\n",
            " 0.76444209 0.1361137  0.16420239 0.60752471]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6FIMUfEmOy3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8cf250dd-1677-437b-a36e-fbfa533696dd"
      },
      "source": [
        "pred101 = np.where(pred1 < 0.5, 0, 1)\n",
        "pred201 = np.where(pred2 < 0.5, 0, 1)\n",
        "pred301 = np.where(pred3 < 0.5, 0, 1)\n",
        "pred501 = np.where(pred5 < 0.5, 0, 1)\n",
        "pred901 = np.where(pred9 < 0.5, 0, 1)\n",
        "print(pred101)\n",
        "print(pred201)\n",
        "print(pred301)\n",
        "print(pred501)\n",
        "print(pred901)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0\n",
            " 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0\n",
            " 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0\n",
            " 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1]\n",
            "[0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0\n",
            " 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0\n",
            " 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1]\n",
            "[0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 0 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1\n",
            " 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0\n",
            " 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0\n",
            " 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1]\n",
            "[0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0\n",
            " 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 0 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1\n",
            " 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0\n",
            " 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0\n",
            " 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1]\n",
            "[0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 0 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1\n",
            " 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0\n",
            " 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0\n",
            " 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noU0nFFzmuCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PassengerId = np.array(test[\"PassengerId\"]).astype(int)\n",
        "                            \n",
        "my_solution1 = pd.DataFrame(pred101, PassengerId, columns = [\"Survived\"])\n",
        "                            \n",
        "my_solution1.to_csv(\"Solution8_2_1.csv\", index_label = [\"PassengerId\"])\n",
        "\n",
        "my_solution2 = pd.DataFrame(pred201, PassengerId, columns = [\"Survived\"])\n",
        "                            \n",
        "my_solution2.to_csv(\"Solution8_2_2.csv\", index_label = [\"PassengerId\"])\n",
        "\n",
        "my_solution3 = pd.DataFrame(pred301, PassengerId, columns = [\"Survived\"])\n",
        "                            \n",
        "my_solution3.to_csv(\"Solution8_2_3.csv\", index_label = [\"PassengerId\"])\n",
        "\n",
        "my_solution5 = pd.DataFrame(pred501, PassengerId, columns = [\"Survived\"])\n",
        "                            \n",
        "my_solution5.to_csv(\"Solution8_2_5.csv\", index_label = [\"PassengerId\"])\n",
        "\n",
        "my_solution9 = pd.DataFrame(pred901, PassengerId, columns = [\"Survived\"])\n",
        "                            \n",
        "my_solution9.to_csv(\"Solution8_2_9.csv\", index_label = [\"PassengerId\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6OKxqQ2nAj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"Solution8_2_1.csv\")\n",
        "files.download(\"Solution8_2_2.csv\")\n",
        "files.download(\"Solution8_2_3.csv\")\n",
        "files.download(\"Solution8_2_5.csv\")\n",
        "files.download(\"Solution8_2_9.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-FE1DEOntQq",
        "colab_type": "text"
      },
      "source": [
        "## 結果\n",
        "\n",
        "* 1   77.033%\n",
        "* 2   77.990%\n",
        "*3  76.076%\n",
        "*5  78.468%\n",
        "*9   79.904%\n",
        "\n",
        "9惜しい...！！"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIuSSpBUtg5s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "dee6b94e-33f8-47be-fbed-c5db4e781a46"
      },
      "source": [
        "gbm9_2 = lgb.train(\n",
        "    param9,\n",
        "    train_data,\n",
        "    valid_sets=eval_data,\n",
        "    num_boost_round=40,\n",
        "    verbose_eval=5,\n",
        ")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5]\tvalid_0's binary_logloss: 0.604092\n",
            "[10]\tvalid_0's binary_logloss: 0.563034\n",
            "[15]\tvalid_0's binary_logloss: 0.530106\n",
            "[20]\tvalid_0's binary_logloss: 0.50444\n",
            "[25]\tvalid_0's binary_logloss: 0.484815\n",
            "[30]\tvalid_0's binary_logloss: 0.467205\n",
            "[35]\tvalid_0's binary_logloss: 0.45494\n",
            "[40]\tvalid_0's binary_logloss: 0.444821\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrJU05oruBwG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3cc8913f-a681-4dac-f55f-22053c479275"
      },
      "source": [
        "pred9_2 = gbm9_2.predict(test_features)\n",
        "pred9_2"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.1528526 , 0.41149786, 0.15957574, 0.22043682, 0.53991243,\n",
              "       0.19485941, 0.5583965 , 0.29831388, 0.64715826, 0.15557951,\n",
              "       0.15350916, 0.15494934, 0.79099269, 0.13150956, 0.77557674,\n",
              "       0.7584811 , 0.16400909, 0.22679844, 0.39667065, 0.50481384,\n",
              "       0.23959284, 0.50435267, 0.76315305, 0.27936676, 0.7675086 ,\n",
              "       0.13053159, 0.80630061, 0.22604887, 0.44654857, 0.18206946,\n",
              "       0.13150956, 0.17955732, 0.4703943 , 0.4703943 , 0.45530985,\n",
              "       0.2149862 , 0.50613556, 0.52375373, 0.17729802, 0.26427116,\n",
              "       0.1859585 , 0.36186586, 0.18793469, 0.67331687, 0.7725562 ,\n",
              "       0.17269973, 0.36523419, 0.20326261, 0.78170541, 0.49718955,\n",
              "       0.45056674, 0.22584486, 0.71697531, 0.51215733, 0.33704825,\n",
              "       0.30258389, 0.13951228, 0.32563823, 0.16270024, 0.80599824,\n",
              "       0.18541376, 0.29974475, 0.20853689, 0.69888297, 0.65902628,\n",
              "       0.74567178, 0.70098666, 0.18041474, 0.50498822, 0.7119431 ,\n",
              "       0.65591099, 0.15463509, 0.41967586, 0.50933422, 0.76654243,\n",
              "       0.41376544, 0.16420239, 0.69212546, 0.2838389 , 0.65591099,\n",
              "       0.6140284 , 0.31919063, 0.16171899, 0.15350916, 0.30287539,\n",
              "       0.19054228, 0.56086496, 0.52413247, 0.5583965 , 0.64065779,\n",
              "       0.53649077, 0.21259906, 0.77986957, 0.16420239, 0.22440643,\n",
              "       0.22489922, 0.73129376, 0.30183371, 0.54318505, 0.17623118,\n",
              "       0.77726609, 0.17355326, 0.20326261, 0.25855985, 0.62269505,\n",
              "       0.1991882 , 0.16578368, 0.20326261, 0.19001171, 0.34452922,\n",
              "       0.18592161, 0.69888297, 0.81199416, 0.69888297, 0.72085328,\n",
              "       0.21198371, 0.21286682, 0.55078949, 0.41376544, 0.75139517,\n",
              "       0.74355853, 0.1930869 , 0.78120997, 0.20618884, 0.20326261,\n",
              "       0.52070633, 0.22249844, 0.64680691, 0.20898691, 0.16115049,\n",
              "       0.31914103, 0.54105593, 0.41660315, 0.19375654, 0.17559086,\n",
              "       0.16681276, 0.26045639, 0.21478174, 0.48219397, 0.18841507,\n",
              "       0.19854986, 0.76914453, 0.32718074, 0.19288943, 0.35252313,\n",
              "       0.25242972, 0.21261864, 0.16244731, 0.3512695 , 0.49536446,\n",
              "       0.75577687, 0.17973816, 0.13716433, 0.45300106, 0.26164004,\n",
              "       0.1854825 , 0.75455395, 0.49907051, 0.36186586, 0.43990128,\n",
              "       0.69888297, 0.6140284 , 0.64322247, 0.18419295, 0.37949277,\n",
              "       0.46533303, 0.24243795, 0.18102358, 0.74324523, 0.53097685,\n",
              "       0.18280207, 0.26214741, 0.15813237, 0.21096819, 0.18841507,\n",
              "       0.75731786, 0.74640677, 0.33128922, 0.80107696, 0.77412897,\n",
              "       0.2838389 , 0.40192229, 0.80463752, 0.20326261, 0.78657197,\n",
              "       0.15928596, 0.74181925, 0.19312152, 0.19854986, 0.20040579,\n",
              "       0.13303424, 0.18225901, 0.49286972, 0.15957574, 0.64322034,\n",
              "       0.15767017, 0.65164872, 0.54069859, 0.16285249, 0.45330021,\n",
              "       0.68535409, 0.52185073, 0.51768411, 0.77635979, 0.16606608,\n",
              "       0.20230833, 0.5583965 , 0.16285249, 0.78632253, 0.15386218,\n",
              "       0.29816725, 0.18419295, 0.22822482, 0.68687222, 0.27207139,\n",
              "       0.21261864, 0.70098666, 0.23008069, 0.73326875, 0.16420239,\n",
              "       0.71322505, 0.16515234, 0.73165756, 0.22593368, 0.72202333,\n",
              "       0.54827882, 0.21883021, 0.69888297, 0.16008105, 0.17681566,\n",
              "       0.23489689, 0.74325689, 0.1879597 , 0.14628558, 0.4002742 ,\n",
              "       0.21324484, 0.32718074, 0.22499475, 0.70644663, 0.74195178,\n",
              "       0.72202333, 0.72987995, 0.29395353, 0.15350916, 0.51561968,\n",
              "       0.32953069, 0.73840987, 0.17432759, 0.75139517, 0.50049499,\n",
              "       0.77831848, 0.21634576, 0.44639909, 0.17256189, 0.17626659,\n",
              "       0.18280207, 0.20326261, 0.18548851, 0.69475629, 0.22164609,\n",
              "       0.1296918 , 0.17092657, 0.7561175 , 0.52675306, 0.29601257,\n",
              "       0.15350916, 0.1841774 , 0.18280207, 0.50613556, 0.19213251,\n",
              "       0.2564261 , 0.20326261, 0.74838947, 0.68535409, 0.21400551,\n",
              "       0.72696231, 0.20058161, 0.14408203, 0.22922279, 0.16578151,\n",
              "       0.48413639, 0.53563233, 0.69888297, 0.62462215, 0.51770477,\n",
              "       0.1361137 , 0.18280207, 0.44596554, 0.35373327, 0.16420239,\n",
              "       0.21445233, 0.5583965 , 0.21096819, 0.5663444 , 0.17280108,\n",
              "       0.17963908, 0.79662932, 0.18206946, 0.28195581, 0.29797339,\n",
              "       0.32276383, 0.42137553, 0.16211586, 0.17256189, 0.69888297,\n",
              "       0.73419372, 0.44167353, 0.53399192, 0.32298093, 0.40831139,\n",
              "       0.19213251, 0.22604887, 0.18280207, 0.5583965 , 0.73161368,\n",
              "       0.69888297, 0.32718074, 0.18998525, 0.19794801, 0.1827212 ,\n",
              "       0.25855985, 0.36500915, 0.32724087, 0.35444082, 0.73546112,\n",
              "       0.18510732, 0.80172917, 0.22140099, 0.22922279, 0.175207  ,\n",
              "       0.72987995, 0.4501491 , 0.21400551, 0.62292596, 0.17963908,\n",
              "       0.40345578, 0.32234426, 0.1816213 , 0.19530982, 0.59159007,\n",
              "       0.18480421, 0.28835814, 0.23691065, 0.77379893, 0.52615847,\n",
              "       0.52707277, 0.21478174, 0.50481384, 0.15705986, 0.73981107,\n",
              "       0.78359618, 0.16606608, 0.22822482, 0.19576461, 0.50035991,\n",
              "       0.24601731, 0.73269567, 0.15350916, 0.20326261, 0.45048631,\n",
              "       0.26164004, 0.78434918, 0.75089183, 0.22043682, 0.80203842,\n",
              "       0.39656986, 0.19054228, 0.4131593 , 0.75724271, 0.30060389,\n",
              "       0.16378435, 0.80378339, 0.2452422 , 0.2079084 , 0.77223578,\n",
              "       0.74152358, 0.47140201, 0.1663742 , 0.2366192 , 0.26164004,\n",
              "       0.20326261, 0.17428915, 0.45480417, 0.53584677, 0.30887156,\n",
              "       0.74568945, 0.21663054, 0.1721074 , 0.21329146, 0.31483028,\n",
              "       0.40531164, 0.76995388, 0.48541008, 0.1633999 , 0.22507697,\n",
              "       0.76273785, 0.17999672, 0.78296369, 0.21825766, 0.28087897,\n",
              "       0.75455395, 0.13040526, 0.78255841, 0.2285504 , 0.37785662,\n",
              "       0.35414129, 0.15982806, 0.3233647 , 0.69888297, 0.52210385,\n",
              "       0.69888297, 0.77713298, 0.42436902, 0.16420239, 0.76444209,\n",
              "       0.1361137 , 0.16420239, 0.60752471])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnNzMMjiuMi9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "362fdd35-23eb-463d-c948-ad66a5cc4bec"
      },
      "source": [
        "pred901_2 = np.where(pred9_2 < 0.5, 0, 1)\n",
        "pred901_2"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
              "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
              "       1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
              "       0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
              "       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
              "       0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
              "       0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo0h7RX_uZmh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_solution9_2 = pd.DataFrame(pred901_2, PassengerId, columns = [\"Survived\"])\n",
        "\n",
        "my_solution9_2.to_csv(\"Solution8_2_9_2.csv\", index_label = [\"PassengerId\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOSbm6AsuoEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(\"Solution8_2_9_2.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alxeT9EWu3qe",
        "colab_type": "text"
      },
      "source": [
        "79.904%...！！"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STdp0tg7vL7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param9_3 = {\n",
        "    'task': 'train',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'learning_rate': 0.03,\n",
        "    'min_data_in_leaf': 20,\n",
        "    'feature_fraction': 0.7,\n",
        "    'num_leaves': 800,\n",
        "    'max_depth': 80,\n",
        "    'n_estimators': 100,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVdQO99uu9xs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "c44ae341-45ca-40c4-884c-98436ad732fd"
      },
      "source": [
        "gbm9_3 = lgb.train(\n",
        "    param9_3,\n",
        "    train_data,\n",
        "    valid_sets=eval_data,\n",
        "    num_boost_round=500,\n",
        "    verbose_eval=20,\n",
        ")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20]\tvalid_0's binary_logloss: 0.50444\n",
            "[40]\tvalid_0's binary_logloss: 0.444821\n",
            "[60]\tvalid_0's binary_logloss: 0.416619\n",
            "[80]\tvalid_0's binary_logloss: 0.398399\n",
            "[100]\tvalid_0's binary_logloss: 0.392926\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p7p6eRYvZfK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b9afd23-3e13-4731-f49f-0bda0b5bbb8c"
      },
      "source": [
        "pred9_3 = gbm9_3.predict(test_features)\n",
        "pred9_3"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.0949215 , 0.35675745, 0.05283515, 0.24733877, 0.57307162,\n",
              "       0.18453363, 0.55707991, 0.25322854, 0.77831492, 0.04576956,\n",
              "       0.08392083, 0.10875018, 0.94338532, 0.03950543, 0.93632946,\n",
              "       0.90504634, 0.10049163, 0.20582418, 0.36459059, 0.51548954,\n",
              "       0.10422143, 0.6279213 , 0.94214191, 0.19599289, 0.92028014,\n",
              "       0.04664336, 0.95428828, 0.20512553, 0.47325604, 0.06964145,\n",
              "       0.05760706, 0.05302218, 0.49867871, 0.44419106, 0.43703375,\n",
              "       0.15910829, 0.49606541, 0.56415332, 0.08362982, 0.26369076,\n",
              "       0.13172924, 0.42652992, 0.12455252, 0.83872924, 0.9357411 ,\n",
              "       0.09038717, 0.40405142, 0.13316676, 0.94366752, 0.55705155,\n",
              "       0.34800173, 0.20164267, 0.86402505, 0.65143691, 0.39785664,\n",
              "       0.12160363, 0.0679533 , 0.24464881, 0.0763142 , 0.96201282,\n",
              "       0.08651992, 0.31420646, 0.11302848, 0.82979959, 0.66632421,\n",
              "       0.91143565, 0.84618556, 0.13230706, 0.59801059, 0.81373873,\n",
              "       0.76773763, 0.06846922, 0.35141484, 0.57074557, 0.92355655,\n",
              "       0.45816418, 0.08882217, 0.75338668, 0.22775148, 0.76773763,\n",
              "       0.80523174, 0.17340918, 0.11629663, 0.08392083, 0.35608593,\n",
              "       0.12188702, 0.70805698, 0.52789234, 0.58167791, 0.83258632,\n",
              "       0.56809371, 0.17120405, 0.92881305, 0.08882217, 0.17191212,\n",
              "       0.16302966, 0.86152023, 0.29013707, 0.62516676, 0.12977617,\n",
              "       0.92165076, 0.13948246, 0.13316676, 0.25988242, 0.70867724,\n",
              "       0.13457998, 0.09665129, 0.13316676, 0.13320456, 0.22163294,\n",
              "       0.10458435, 0.82858522, 0.96125403, 0.83306692, 0.85427534,\n",
              "       0.15475734, 0.11194486, 0.61059267, 0.40257141, 0.88139832,\n",
              "       0.8779304 , 0.11871715, 0.93554771, 0.14403432, 0.13316676,\n",
              "       0.5745815 , 0.16667288, 0.74265268, 0.15874613, 0.06184621,\n",
              "       0.34112409, 0.41478363, 0.39512478, 0.09428296, 0.10180839,\n",
              "       0.08274116, 0.29210913, 0.17430716, 0.47157124, 0.08458339,\n",
              "       0.09310674, 0.95086287, 0.16576934, 0.14234966, 0.43887104,\n",
              "       0.14477101, 0.19078312, 0.06400016, 0.40368438, 0.46165728,\n",
              "       0.91405238, 0.11196619, 0.04525351, 0.43009217, 0.09378617,\n",
              "       0.10676948, 0.90100948, 0.54149119, 0.4607985 , 0.48795288,\n",
              "       0.82858522, 0.80523174, 0.80747838, 0.07588144, 0.31403548,\n",
              "       0.47113482, 0.20276049, 0.05903123, 0.90169384, 0.59430208,\n",
              "       0.0806915 , 0.29392398, 0.05682544, 0.13573073, 0.08693736,\n",
              "       0.91736156, 0.92539359, 0.18502336, 0.95112818, 0.92576533,\n",
              "       0.22775148, 0.46426069, 0.95114768, 0.13316676, 0.94268042,\n",
              "       0.06174225, 0.89526189, 0.07508469, 0.09317647, 0.13281868,\n",
              "       0.07622079, 0.11797752, 0.66578142, 0.05257185, 0.82678241,\n",
              "       0.10614566, 0.85971087, 0.57317623, 0.05315685, 0.46062496,\n",
              "       0.81805559, 0.72061714, 0.61352607, 0.8988058 , 0.06426496,\n",
              "       0.1570132 , 0.57324741, 0.05690639, 0.93976669, 0.06078354,\n",
              "       0.29954017, 0.07588144, 0.12542385, 0.77744897, 0.17979434,\n",
              "       0.16071806, 0.84618556, 0.08432389, 0.86869421, 0.08882217,\n",
              "       0.86194648, 0.07031171, 0.8894185 , 0.18115016, 0.87013357,\n",
              "       0.60840894, 0.14593091, 0.82858522, 0.05830902, 0.09951006,\n",
              "       0.2154774 , 0.91076835, 0.11563837, 0.07548174, 0.41513822,\n",
              "       0.14656138, 0.17098812, 0.20313456, 0.83870577, 0.88222954,\n",
              "       0.87013357, 0.88792631, 0.32365918, 0.08392083, 0.64910811,\n",
              "       0.26324441, 0.89877837, 0.0879655 , 0.88139832, 0.47975838,\n",
              "       0.91283301, 0.15451614, 0.52267134, 0.07202478, 0.1574711 ,\n",
              "       0.0806915 , 0.13316676, 0.12353688, 0.84410703, 0.16182769,\n",
              "       0.04833799, 0.09491875, 0.90887991, 0.55923518, 0.25574763,\n",
              "       0.08392083, 0.06045779, 0.0806915 , 0.49606541, 0.09905422,\n",
              "       0.23553143, 0.13316676, 0.8812956 , 0.81038328, 0.13344712,\n",
              "       0.86393038, 0.15273158, 0.06853682, 0.11483835, 0.0616644 ,\n",
              "       0.48674837, 0.7571274 , 0.82858522, 0.67571757, 0.56597584,\n",
              "       0.03924658, 0.08771871, 0.41196081, 0.28676469, 0.08882217,\n",
              "       0.14208288, 0.54677682, 0.13573073, 0.49160103, 0.09358355,\n",
              "       0.1225042 , 0.93539907, 0.06964145, 0.22139111, 0.29216251,\n",
              "       0.34719124, 0.49401271, 0.07203476, 0.07529029, 0.82858522,\n",
              "       0.87321878, 0.34328751, 0.73814088, 0.14975508, 0.32071342,\n",
              "       0.09905422, 0.20512553, 0.0806915 , 0.54029971, 0.88086752,\n",
              "       0.82858522, 0.18561347, 0.08909895, 0.19268091, 0.05410349,\n",
              "       0.25988242, 0.35142965, 0.30229878, 0.48259161, 0.92956877,\n",
              "       0.09663064, 0.94595109, 0.20725191, 0.13754996, 0.0685635 ,\n",
              "       0.89805252, 0.49508968, 0.13344712, 0.71338747, 0.14175833,\n",
              "       0.33595041, 0.40662759, 0.08161452, 0.10680271, 0.74579313,\n",
              "       0.08244349, 0.18028897, 0.12730071, 0.94196485, 0.71229157,\n",
              "       0.57522609, 0.17430716, 0.54035329, 0.0502368 , 0.87421296,\n",
              "       0.94125055, 0.06426496, 0.12542385, 0.10700449, 0.53009294,\n",
              "       0.21704266, 0.86799888, 0.086696  , 0.13316676, 0.4556747 ,\n",
              "       0.09090089, 0.9431367 , 0.90320289, 0.24733877, 0.95372504,\n",
              "       0.33988522, 0.12188702, 0.42064989, 0.93467786, 0.28166454,\n",
              "       0.06876689, 0.95662411, 0.10513687, 0.15787514, 0.93187876,\n",
              "       0.92103511, 0.44927542, 0.06294198, 0.08633677, 0.11664246,\n",
              "       0.13316676, 0.12045092, 0.45565833, 0.58638619, 0.36517441,\n",
              "       0.88865604, 0.14241675, 0.06336028, 0.13462384, 0.18783944,\n",
              "       0.33537258, 0.93312115, 0.6581142 , 0.10394988, 0.08081358,\n",
              "       0.89529611, 0.10617047, 0.94002465, 0.14861438, 0.16182932,\n",
              "       0.90022634, 0.044904  , 0.94502016, 0.14731573, 0.41434706,\n",
              "       0.33056028, 0.05793766, 0.26828882, 0.82858522, 0.55463528,\n",
              "       0.82858522, 0.93158429, 0.36395621, 0.08882217, 0.9136302 ,\n",
              "       0.04029899, 0.08882217, 0.79250716])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BerFb6l5vgWQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "3612071c-9919-4f7b-d795-51222c9ae03d"
      },
      "source": [
        "pred901_3 = np.where(pred9_3 < 0.5, 0, 1)\n",
        "pred901_3"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
              "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
              "       1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
              "       0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
              "       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
              "       0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
              "       0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
              "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZK92FKYvnvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_solution9_3 = pd.DataFrame(pred901_3, PassengerId, columns = [\"Survived\"])\n",
        "\n",
        "my_solution9_3.to_csv(\"Solution8_2_9_3.csv\", index_label = [\"PassengerId\"])\n",
        "\n",
        "files.download(\"Solution8_2_9_3.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3W4QFP6v3hV",
        "colab_type": "text"
      },
      "source": [
        "80.382%！\n",
        "ついに8割超えた！！！"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOeqOq56wcaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRaAiUkmwk3m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "af164ca2-2951-430c-f791-9d8877c06a25"
      },
      "source": [
        "fig, axs = plt.subplots(figsize=[50, 4])\n",
        "importances = pd.DataFrame({'features': gbm9_3.feature_name(), \n",
        "                            'importance': gbm9_3.feature_importance()}).sort_values('importance', ascending=False)\n",
        "axs.bar(x=np.arange(len(importances)), height=importances['importance'])\n",
        "axs.set_xticks(np.arange(len(importances)))\n",
        "axs.set_xticklabels(importances['features'])\n",
        "axs.set_ylabel('Feature importance (# times used to split)')\n",
        "axs.set_title('Feature importance')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAACx8AAAEJCAYAAABPd3tPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZIkSZIkSZIkSfPy/xwOajmrRPNLAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 3600x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLFunCV8xjVH",
        "colab_type": "text"
      },
      "source": [
        "重要度は順に年齢、料金、チケットクラス、同乗している兄弟/配偶者の数、同乗している親/子の数、出港地S、客室番号の欠損、Mrの敬称、女性であること、出港地C、出港地Q、客室番号C、となった。"
      ]
    }
  ]
}